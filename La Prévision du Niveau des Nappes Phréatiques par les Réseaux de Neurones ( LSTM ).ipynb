{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 1. Visualisation et étude de corrélation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non normalisés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "csv_files_path = './Data/'\n",
    "csv_files = [f for f in os.listdir(csv_files_path) if f.endswith('.csv')]\n",
    "example_files = csv_files[:3]\n",
    "example_data = {file: pd.read_csv(os.path.join(csv_files_path, file)) for file in example_files}\n",
    "example_data[example_files[2]].head()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def affichage_plot(data, title, directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory) \n",
    "    \n",
    "    data['year'] = pd.to_datetime(data['date']).dt.year\n",
    "    data['year_str'] = data['year'].astype(str)\n",
    "    unique_dates = data.drop_duplicates(subset='year') \n",
    "\n",
    "    for column in data.columns[1:-2]:  \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(data['date'], data[column], label=column, marker='.', linestyle='-')\n",
    "        plt.title(f'{column} over time for {title}')\n",
    "        plt.xlabel('Year')\n",
    "        plt.ylabel(column)\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.xticks(unique_dates['date'], unique_dates['year_str'], rotation=45)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        file_name = os.path.join(directory, f\"{title}_{column}.png\")\n",
    "        plt.savefig(file_name)\n",
    "        \n",
    "        plt.close() \n",
    "\n",
    "output_directory = \"plots\"\n",
    "for file, data in example_data.items():\n",
    "    affichage_plot(data, file, output_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def affichage_plot(data, title, directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    data['year'] = pd.to_datetime(data['date']).dt.year\n",
    "    data['year_str'] = data['year'].astype(str)\n",
    "    unique_dates = data.drop_duplicates(subset='year')  \n",
    "\n",
    "    for column in data.columns[1:-2]:  \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(data['date'], data[column], label=column, marker='.', linestyle='-')\n",
    "        plt.title(f'{column} over time for {title}')\n",
    "        plt.xlabel('Year')\n",
    "        plt.ylabel(column)\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.xticks(unique_dates['date'], unique_dates['year_str'], rotation=45)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "       \n",
    "        file_name = os.path.join(directory, f\"{title}_{column}_Normalized.png\")\n",
    "        plt.savefig(file_name)\n",
    "        \n",
    "        plt.close()\n",
    "\n",
    "def normalize_data_cleaned(data):\n",
    "    data_cleaned = data.dropna()  # Supprimer les valeurs manquantes\n",
    "    numerical_columns = data_cleaned.select_dtypes(include=[np.number]).columns  \n",
    "    for column in numerical_columns:\n",
    "        mean = data_cleaned[column].mean()\n",
    "        std = data_cleaned[column].std()\n",
    "        if std != 0:\n",
    "            data_cleaned[column] = (data_cleaned[column] - mean) / std\n",
    "        else:\n",
    "            data_cleaned[column] = 0  # Normaliser en utilisant la moyenne et l'écart-type\n",
    "    return data_cleaned\n",
    "\n",
    "# Normaliser les données pour chaque fichier\n",
    "normalized_data_cleaned = {file: normalize_data_cleaned(df.copy()) for file, df in example_data.items()}\n",
    "\n",
    "for file, data in normalized_data_cleaned.items():\n",
    "    affichage_plot(data, file, output_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrélation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def calculate_ccc(data, var1, var2):\n",
    "    \"\"\"\n",
    "    Calculer le Coefficient de Corrélation de Concordance entre deux variables.\n",
    "    \"\"\"\n",
    "    clean_data = data.dropna(subset=[var1, var2])  # Supprimer les valeurs manquantes pour les variables spécifiées\n",
    "    mean_var1 = clean_data[var1].mean()\n",
    "    mean_var2 = clean_data[var2].mean()\n",
    "    var1_mean_diff = clean_data[var1] - mean_var1\n",
    "    var2_mean_diff = clean_data[var2] - mean_var2\n",
    "    \n",
    "    covariance = (var1_mean_diff * var2_mean_diff).mean()\n",
    "    var1_variance = clean_data[var1].var()\n",
    "    var2_variance = clean_data[var2].var()\n",
    "    \n",
    "    correlation = covariance / (var1_variance * var2_variance)**0.5\n",
    "    mean_diff_ratio = 2 * covariance / (var1_variance + var2_variance + (mean_var1 - mean_var2)**2)\n",
    "    \n",
    "    ccc = correlation * mean_diff_ratio\n",
    "    return ccc\n",
    "\n",
    "variables = ['ET', 'NDVI', 'P', 'T']\n",
    "\n",
    "ccc_results = {}\n",
    "for file, data in example_data.items():\n",
    "    ccc_results[file] = {}\n",
    "    for var in variables:\n",
    "        #données non normalisées\n",
    "        ccc_results[file][f'GWL vs {var} (non-normalized)'] = calculate_ccc(data, 'GWL', var)\n",
    "        #données normalisées\n",
    "        ccc_results[file][f'GWL vs {var} (normalized)'] = calculate_ccc(normalized_data_cleaned[file], 'GWL', var)\n",
    "\n",
    "ccc_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 2. Prédiction du niveau des nappes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prétraitement de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Définir les méthodes d'imputation\n",
    "def knn_imputation(df, n_neighbors=5):\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "    df_imputed = df.copy()\n",
    "    df_imputed[numeric_cols] = imputer.fit_transform(df[numeric_cols])\n",
    "    return df_imputed\n",
    "\n",
    "def linear_interpolation_imputation(df):\n",
    "    df_imputed = df.copy()\n",
    "    df_imputed = df_imputed.interpolate(method='linear')\n",
    "    return df_imputed\n",
    "\n",
    "def linear_knn_imputation(df, n_neighbors=5):\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    df_interpolated = df[numeric_cols].copy()\n",
    "    df_interpolated = df_interpolated.interpolate(method='linear')\n",
    "    \n",
    "    imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "    df_imputed = df_interpolated.copy()\n",
    "    df_imputed.iloc[:, :] = imputer.fit_transform(df_interpolated)\n",
    "    \n",
    "    df_combined = df.copy()\n",
    "    df_combined[numeric_cols] = df_imputed\n",
    "    \n",
    "    return df_combined\n",
    "\n",
    "# Fonction pour masquer une partie des valeurs non nulles pour l'évaluation\n",
    "def mask_values(df, column, mask_fraction=0.2):\n",
    "    df_copy = df.copy()\n",
    "    non_null_indices = df_copy[df_copy[column].notna()].index\n",
    "    mask_size = int(len(non_null_indices) * mask_fraction)\n",
    "    mask_indices = np.random.choice(non_null_indices, mask_size, replace=False)\n",
    "    df_copy.loc[mask_indices, column] = np.nan\n",
    "    return df_copy, mask_indices\n",
    "\n",
    "# Fonction pour évaluer les méthodes d'imputation\n",
    "def evaluate_imputation_on_masked(df, original_df, mask_indices, method_name):\n",
    "    rmse = sqrt(mean_squared_error(original_df.loc[mask_indices]['GWL'], df.loc[mask_indices]['GWL']))\n",
    "    return rmse\n",
    "\n",
    "# Fonction pour appliquer l'imputation et s'assurer qu'il n'y a plus de NaN\n",
    "def apply_and_evaluate(method, method_name, masked_df, original_df, mask_indices, n_neighbors=5):\n",
    "    if method_name == \"KNN\":\n",
    "        imputed_df = knn_imputation(masked_df, n_neighbors)\n",
    "    elif method_name == \"Linear\":\n",
    "        imputed_df = linear_interpolation_imputation(masked_df)\n",
    "    elif method_name == \"KNN + Linear\":\n",
    "        imputed_df = linear_knn_imputation(masked_df, n_neighbors)\n",
    "    \n",
    "    imputed_df.fillna(imputed_df.mean(), inplace=True)\n",
    "    \n",
    "    rmse = evaluate_imputation_on_masked(imputed_df, original_df, mask_indices, method_name)\n",
    "    return rmse\n",
    "\n",
    "# Fonction pour traiter et évaluer les fichiers\n",
    "def process_and_evaluate_files(source_dir, n_neighbors=5):\n",
    "    results = {}\n",
    "    errors = []\n",
    "    for filename in os.listdir(source_dir):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            try:\n",
    "                filepath = os.path.join(source_dir, filename)\n",
    "                df = pd.read_csv(filepath)\n",
    "                \n",
    "                masked_df, mask_indices = mask_values(df, 'GWL')\n",
    "                \n",
    "                rmse_knn = apply_and_evaluate(knn_imputation, \"KNN\", masked_df, df, mask_indices, n_neighbors)\n",
    "                rmse_linear = apply_and_evaluate(linear_interpolation_imputation, \"Linear\", masked_df, df, mask_indices, n_neighbors)\n",
    "                rmse_linear_knn = apply_and_evaluate(linear_knn_imputation, \"KNN + Linear\", masked_df, df, mask_indices, n_neighbors)\n",
    "                \n",
    "                results[filename] = {\n",
    "                    \"KNN\": rmse_knn,\n",
    "                    \"Linear\": rmse_linear,\n",
    "                    \"KNN + Linear\": rmse_linear_knn\n",
    "                }\n",
    "            except Exception as e:\n",
    "                errors.append((filename, str(e)))\n",
    "    \n",
    "    return results, errors\n",
    "\n",
    "source_directory = './Data/'\n",
    "\n",
    "results, errors = process_and_evaluate_files(source_directory, n_neighbors=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(results).T\n",
    "\n",
    "# Identifier la meilleure approche\n",
    "df['Best_Approach'] = df.idxmin(axis=1)\n",
    "mean_performance = df.mean()\n",
    "best_overall_approach = mean_performance.idxmin()\n",
    "\n",
    "print(\"Meilleure approche pour chaque fichier :\")\n",
    "print(df['Best_Approach'].value_counts())\n",
    "print(\"\\nPerformance moyenne de chaque méthode :\")\n",
    "print(mean_performance)\n",
    "print(\"\\nMeilleure approche globale :\")\n",
    "print(best_overall_approach)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "import numpy as np\n",
    "\n",
    "def linear_knn_imputation(df):\n",
    "    # Sélectionner uniquement les colonnes numériques\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    df_interpolated = df[numeric_cols].copy()\n",
    "    df_interpolated = df_interpolated.interpolate(method='linear')\n",
    "    \n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    df_imputed = df_interpolated.copy()\n",
    "    df_imputed.iloc[:, :] = imputer.fit_transform(df_interpolated)\n",
    "    \n",
    "    # Ajouter du bruit aléatoire pour éviter les valeurs identiques\n",
    "    noise = np.random.normal(0, 1e-5, df_imputed.shape)\n",
    "    df_imputed += noise\n",
    "    \n",
    "    df_combined = df.copy()\n",
    "    df_combined[numeric_cols] = df_imputed\n",
    "    \n",
    "    # Supprimer les lignes avec des valeurs imputées identiques\n",
    "    df_combined = df_combined.loc[~df_combined.duplicated(subset=numeric_cols, keep=False)]\n",
    "    \n",
    "    return df_combined\n",
    "\n",
    "def process_files(source_dir, dest_dir):\n",
    "    \n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "    \n",
    "    for filename in os.listdir(source_dir):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            try:\n",
    "                filepath = os.path.join(source_dir, filename)\n",
    "                df = pd.read_csv(filepath)\n",
    "                \n",
    "                # Appliquer l'imputation Linéaire + KNN\n",
    "                df_imputed = linear_knn_imputation(df)\n",
    "                \n",
    "                # Sauvegarder\n",
    "                output_filepath = os.path.join(dest_dir, filename)\n",
    "                df_imputed.to_csv(output_filepath, index=False)\n",
    "                print(f\"Processed and saved: {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {filename}: {e}\")\n",
    "\n",
    "source_directory = './Data/'  \n",
    "destination_directory = './Data1/'  \n",
    "\n",
    "process_files(source_directory, destination_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "#Pour supprimer les valeurs Null au lieu de les remplacer\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def process_files(source_dir, dest_dir):\n",
    "    \n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "    \n",
    "    \n",
    "    for filename in os.listdir(source_dir):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            try:\n",
    "                filepath = os.path.join(source_dir, filename)\n",
    "                df = pd.read_csv(filepath)\n",
    "                \n",
    "                # Supprimer les lignes où GWL est nul\n",
    "                df_cleaned = df.dropna(subset=['GWL'])\n",
    "                \n",
    "\n",
    "                output_filepath = os.path.join(dest_dir, filename)\n",
    "                df_cleaned.to_csv(output_filepath, index=False)\n",
    "                print(f\"Processed and saved: {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {filename}: {e}\")\n",
    "\n",
    "source_directory = './Data/'  \n",
    "destination_directory = './Data/' \n",
    "\n",
    "process_files(source_directory, destination_directory)\"\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle spécifique à un puits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Fonction de prétraitement des données\n",
    "def preprocess_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Trier par date\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    data.sort_values('date', inplace=True)\n",
    "\n",
    "    # Extraire les features et la cible\n",
    "    features = data[['P', 'T', 'ET', 'NDVI']]\n",
    "    target = data['GWL']\n",
    "\n",
    "    # Normaliser les features\n",
    "    feature_scaler = MinMaxScaler()\n",
    "    features_scaled = feature_scaler.fit_transform(features)\n",
    "\n",
    "    # Normaliser la cible\n",
    "    target_scaler = MinMaxScaler()\n",
    "    target_scaled = target_scaler.fit_transform(target.values.reshape(-1, 1))\n",
    "\n",
    "    # Créer des séquences pour les prédictions à 6 mois\n",
    "    sequence_length = 6\n",
    "    sequences = []\n",
    "    targets = []\n",
    "\n",
    "    for i in range(len(features_scaled) - sequence_length):\n",
    "        sequences.append(features_scaled[i:i+sequence_length])\n",
    "        targets.append(target_scaled[i+1:i+1+sequence_length])\n",
    "\n",
    "    sequences = np.array(sequences)\n",
    "    targets = np.array(targets).reshape(-1, sequence_length)\n",
    "\n",
    "    # Diviser les données\n",
    "    train_size = len(sequences) - 24  # 12 mois pour validation et test\n",
    "    val_size = 12\n",
    "    test_size = 12\n",
    "\n",
    "    train_features = sequences[:train_size]\n",
    "    train_target = targets[:train_size]\n",
    "\n",
    "    val_features = sequences[train_size:train_size+val_size]\n",
    "    val_target = targets[train_size:train_size+val_size]\n",
    "\n",
    "    test_features = sequences[train_size+val_size:]\n",
    "    test_target = targets[train_size+val_size:]\n",
    "\n",
    "    # Convertir en tenseurs\n",
    "    train_features_tensor = torch.tensor(train_features, dtype=torch.float32)\n",
    "    train_target_tensor = torch.tensor(train_target, dtype=torch.float32)\n",
    "\n",
    "    val_features_tensor = torch.tensor(val_features, dtype=torch.float32)\n",
    "    val_target_tensor = torch.tensor(val_target, dtype=torch.float32)\n",
    "\n",
    "    test_features_tensor = torch.tensor(test_features, dtype=torch.float32)\n",
    "    test_target_tensor = torch.tensor(test_target, dtype=torch.float32)\n",
    "\n",
    "    return train_features_tensor, train_target_tensor, val_features_tensor, val_target_tensor, test_features_tensor, test_target_tensor, target_scaler\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Fonction pour entraîner le modèle\n",
    "def train_model(train_loader, val_loader, model, criterion, optimizer, num_epochs, patience):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for features, target in train_loader:\n",
    "            output = model(features)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for features, target in val_loader:\n",
    "                output = model(features)\n",
    "                loss = criterion(output, target)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            best_model = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch + 1}')\n",
    "            model.load_state_dict(best_model)\n",
    "            break\n",
    "    \n",
    "    model.load_state_dict(best_model)\n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Fonction pour évaluer le modèle\n",
    "def evaluate_model(test_loader, model, target_scaler):\n",
    "    model.eval()\n",
    "    mae = 0\n",
    "    with torch.no_grad():\n",
    "        for features, target in test_loader:\n",
    "            output = model(features)\n",
    "            output = target_scaler.inverse_transform(output.numpy())\n",
    "            target = target_scaler.inverse_transform(target.numpy())\n",
    "\n",
    "            mae += np.mean(np.abs(output - target))\n",
    "    \n",
    "    mae /= len(test_loader)\n",
    "    return mae\n",
    "\n",
    "data_dir = './data1'\n",
    "files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.csv')][:20]\n",
    "\n",
    "# Hyperparamètres\n",
    "hidden_size = 50\n",
    "num_layers = 2\n",
    "output_size = 6  # Prédiction sur six mois\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "patience = 10  # Patience pour l'arrêt\n",
    "\n",
    "mae_list = []\n",
    "\n",
    "for file_path in files:\n",
    "    train_features_tensor, train_target_tensor, val_features_tensor, val_target_tensor, test_features_tensor, test_target_tensor, target_scaler = preprocess_data(file_path)\n",
    "\n",
    "    # Créer le DataLoader\n",
    "    train_dataset = TensorDataset(train_features_tensor, train_target_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    val_dataset = TensorDataset(val_features_tensor, val_target_tensor)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    test_dataset = TensorDataset(test_features_tensor, test_target_tensor)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    input_size = train_features_tensor.shape[2]\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_losses, val_losses = train_model(train_loader, val_loader, model, criterion, optimizer, num_epochs, patience)\n",
    "\n",
    "    test_mae = evaluate_model(test_loader, model, target_scaler)\n",
    "    mae_list.append(test_mae)\n",
    "    print(f'File: {file_path}, Test MAE: {test_mae:.4f}')\n",
    "\n",
    "# Calculer le MAE minimum et moyen\n",
    "min_mae = np.min(mae_list)\n",
    "avg_mae = np.mean(mae_list)\n",
    "\n",
    "print(f'Minimum MAE: {min_mae:.4f}')\n",
    "print(f'Average MAE: {avg_mae:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle général simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Fonction de prétraitement des données\n",
    "def preprocess_data(files):\n",
    "    train_features_list = []\n",
    "    train_target_list = []\n",
    "    test_features_list = []\n",
    "    test_target_list = []\n",
    "\n",
    "    for file_path in files:\n",
    "        data = pd.read_csv(file_path)  \n",
    "\n",
    "        data['date'] = pd.to_datetime(data['date']) \n",
    "        data.sort_values('date', inplace=True)  # Trier les données par date\n",
    "\n",
    "        features = data[['P', 'T', 'ET', 'NDVI']]  # Extraire les features\n",
    "        target = data['GWL']  # Extraire la cible\n",
    "\n",
    "        feature_scaler = MinMaxScaler()  # Initialiser le scaler pour les features\n",
    "        features_scaled = feature_scaler.fit_transform(features)  # Normaliser les features\n",
    "\n",
    "        target_scaler = MinMaxScaler()  # Initialiser le scaler pour la cible\n",
    "        target_scaled = target_scaler.fit_transform(target.values.reshape(-1, 1))  # Normaliser la cible\n",
    "\n",
    "        sequence_length = 6  # Longueur des séquences pour les prédictions à 6 mois\n",
    "        num_data_points = len(features_scaled)  # Nombre de points de données\n",
    "\n",
    "        train_end = num_data_points - 12  # Définir la fin des données d'entraînement\n",
    "        #test_start = train_end  # Début des données de test\n",
    "\n",
    "        # S'assurer qu'il y a suffisamment de points de données pour l'entraînement et le test\n",
    "        if train_end < sequence_length:\n",
    "            continue\n",
    "\n",
    "        file_train_features_list = []\n",
    "        file_train_target_list = []\n",
    "        file_test_features_list = []\n",
    "        file_test_target_list = []\n",
    "\n",
    "        # Créer des séquences pour les données d'entraînement\n",
    "        for i in range(sequence_length, train_end + 1):\n",
    "            file_train_features_list.append(features_scaled[i-sequence_length:i])\n",
    "            file_train_target_list.append(target_scaled[i-sequence_length+1:i+1])\n",
    "\n",
    "        # Créer des séquences pour les données de test\n",
    "        for i in range(train_end, num_data_points - sequence_length + 1):\n",
    "            file_test_features_list.append(features_scaled[i-sequence_length:i])\n",
    "            file_test_target_list.append(target_scaled[i-sequence_length+1:i+1])\n",
    "\n",
    "        # Ajouter les séquences de chaque fichier à la liste principale\n",
    "        train_features_list.extend(file_train_features_list)\n",
    "        train_target_list.extend(file_train_target_list)\n",
    "        test_features_list.extend(file_test_features_list)\n",
    "        test_target_list.extend(file_test_target_list)\n",
    "\n",
    "    # Convertir les listes en arrays numpy\n",
    "    train_features_array = np.array(train_features_list)\n",
    "    train_target_array = np.array(train_target_list).reshape(-1, sequence_length)\n",
    "\n",
    "    test_features_array = np.array(test_features_list)\n",
    "    test_target_array = np.array(test_target_list).reshape(-1, sequence_length)\n",
    "\n",
    "    # Convertir les arrays numpy en tenseurs PyTorch\n",
    "    train_features_tensor = torch.tensor(train_features_array, dtype=torch.float32)\n",
    "    train_target_tensor = torch.tensor(train_target_array, dtype=torch.float32)\n",
    "\n",
    "    test_features_tensor = torch.tensor(test_features_array, dtype=torch.float32)\n",
    "    test_target_tensor = torch.tensor(test_target_array, dtype=torch.float32)\n",
    "\n",
    "    return train_features_tensor, train_target_tensor, test_features_tensor, test_target_tensor, target_scaler\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Fonction pour entraîner le modèle\n",
    "def train_model(train_loader, model, criterion, optimizer, num_epochs, model_save_path):\n",
    "    train_losses = []\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for features, target in train_loader:\n",
    "            output = model(features)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}')\n",
    "\n",
    "        # Sauvegarder le meilleur modèle\n",
    "        if train_loss < best_loss:\n",
    "            best_loss = train_loss\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "    \n",
    "    return train_losses\n",
    "\n",
    "data_dir = './data1'\n",
    "files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.csv')]\n",
    "\n",
    "hidden_size = 50\n",
    "num_layers = 2\n",
    "output_size = 6  # Prédiction sur six mois\n",
    "num_epochs = 65\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "\n",
    "train_features_tensor, train_target_tensor, test_features_tensor, test_target_tensor, target_scaler = preprocess_data(files)\n",
    "\n",
    "# Créer le DataLoader\n",
    "train_dataset = TensorDataset(train_features_tensor, train_target_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_features_tensor, test_target_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "input_size = train_features_tensor.shape[2]\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "model_save_path = './model/best_model.pth'\n",
    "if not os.path.exists(os.path.dirname(model_save_path)):\n",
    "    os.makedirs(os.path.dirname(model_save_path))\n",
    "\n",
    "train_losses = train_model(train_loader, model, criterion, optimizer, num_epochs, model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture du modèle sauvegarder\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "\n",
    "# Evaluation\n",
    "def evaluate_model(test_loader, model, target_scaler):\n",
    "    model.eval()\n",
    "    mae_list = []\n",
    "    errors = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, target in test_loader:\n",
    "            output = model(features)\n",
    "            output = target_scaler.inverse_transform(output.numpy())\n",
    "            target = target_scaler.inverse_transform(target.numpy())\n",
    "\n",
    "            mae = np.mean(np.abs(output - target))\n",
    "            mae_list.append(mae)\n",
    "\n",
    "            errors.append(output - target)\n",
    "    \n",
    "    errors = np.concatenate(errors)\n",
    "    return np.mean(mae_list), np.std(errors), np.min(errors), np.max(errors)\n",
    "\n",
    "\n",
    "test_mae, std_error, min_error, max_error = evaluate_model(test_loader, model, target_scaler)\n",
    "print(f'Final Test MAE: {test_mae:.4f}, Std Error: {std_error:.4f}, Min Error: {min_error:.4f}, Max Error: {max_error:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle général fine-tuned par région"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Prétraitement des données pour une région spécifique\n",
    "def preprocess_region_data(files):\n",
    "    train_features_list = []\n",
    "    train_target_list = []\n",
    "    test_features_list = []\n",
    "    test_target_list = []\n",
    "\n",
    "    for file_path in files:\n",
    "        data = pd.read_csv(file_path)\n",
    "        data['date'] = pd.to_datetime(data['date'])\n",
    "        data.sort_values('date', inplace=True)\n",
    "\n",
    "        features = data[['P', 'T', 'ET', 'NDVI']]\n",
    "        target = data['GWL']\n",
    "\n",
    "        feature_scaler = MinMaxScaler()\n",
    "        features_scaled = feature_scaler.fit_transform(features)\n",
    "\n",
    "        target_scaler = MinMaxScaler()\n",
    "        target_scaled = target_scaler.fit_transform(target.values.reshape(-1, 1))\n",
    "\n",
    "        sequence_length = 6\n",
    "        num_data_points = len(features_scaled)\n",
    "        train_end = num_data_points - 12\n",
    "\n",
    "        if train_end < sequence_length:\n",
    "            continue\n",
    "\n",
    "        file_train_features_list = []\n",
    "        file_train_target_list = []\n",
    "        file_test_features_list = []\n",
    "        file_test_target_list = []\n",
    "\n",
    "        for i in range(sequence_length, train_end + 1):\n",
    "            file_train_features_list.append(features_scaled[i-sequence_length:i])\n",
    "            file_train_target_list.append(target_scaled[i-sequence_length+1:i+1])\n",
    "\n",
    "        for i in range(train_end, num_data_points - sequence_length + 1):\n",
    "            file_test_features_list.append(features_scaled[i-sequence_length:i])\n",
    "            file_test_target_list.append(target_scaled[i-sequence_length+1:i+1])\n",
    "\n",
    "        train_features_list.extend(file_train_features_list)\n",
    "        train_target_list.extend(file_train_target_list)\n",
    "        test_features_list.extend(file_test_features_list)\n",
    "        test_target_list.extend(file_test_target_list)\n",
    "\n",
    "    train_features_array = np.array(train_features_list)\n",
    "    train_target_array = np.array(train_target_list).reshape(-1, sequence_length)\n",
    "\n",
    "    test_features_array = np.array(test_features_list)\n",
    "    test_target_array = np.array(test_target_list).reshape(-1, sequence_length)\n",
    "\n",
    "    train_features_tensor = torch.tensor(train_features_array, dtype=torch.float32)\n",
    "    train_target_tensor = torch.tensor(train_target_array, dtype=torch.float32)\n",
    "\n",
    "    test_features_tensor = torch.tensor(test_features_array, dtype=torch.float32)\n",
    "    test_target_tensor = torch.tensor(test_target_array, dtype=torch.float32)\n",
    "\n",
    "    return train_features_tensor, train_target_tensor, test_features_tensor, test_target_tensor, target_scaler\n",
    "\n",
    "# Fine-tuning du modèle\n",
    "def fine_tune_model(train_loader, model, criterion, optimizer, num_epochs, model_save_path):\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for features, target in train_loader:\n",
    "            output = model(features)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        if train_loss < best_loss:\n",
    "            best_loss = train_loss\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f} (Best: {best_loss:.4f})')\n",
    "\n",
    "# Évaluation du modèle sur les données de test\n",
    "def evaluate_model(test_loader, model, target_scaler):\n",
    "    model.eval()\n",
    "    mae_list = []\n",
    "    errors = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, target in test_loader:\n",
    "            output = model(features)\n",
    "            output = target_scaler.inverse_transform(output.numpy())\n",
    "            target = target_scaler.inverse_transform(target.numpy())\n",
    "\n",
    "            mae = np.mean(np.abs(output - target))\n",
    "            mae_list.append(mae)\n",
    "\n",
    "            errors.append(output - target)\n",
    "    \n",
    "    errors = np.concatenate(errors)\n",
    "    return np.mean(mae_list), np.std(errors), np.min(errors), np.max(errors)\n",
    "\n",
    "input_size = 4\n",
    "hidden_size = 50\n",
    "num_layers = 2\n",
    "output_size = 6\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "\n",
    "# Lire OUVRAGES.csv pour obtenir les identifiants de puits et les régions\n",
    "ouvrages_file_path = './OUVRAGES.csv'\n",
    "ouvrages_data = pd.read_csv(ouvrages_file_path)\n",
    "\n",
    "well_id_column = 'Ouvrage'\n",
    "region_column = 'Region'\n",
    "\n",
    "vic_wells = ouvrages_data[ouvrages_data[region_column] == 'VIC'][well_id_column].tolist()\n",
    "qld_wells = ouvrages_data[ouvrages_data[region_column] == 'QLD'][well_id_column].tolist()\n",
    "\n",
    "# Préparer la liste des fichiers CSV correspondant aux puits pour chaque région\n",
    "data_dir = './data1'\n",
    "vic_files = [os.path.join(data_dir, f\"{well_id}.csv\") for well_id in vic_wells if os.path.isfile(os.path.join(data_dir, f\"{well_id}.csv\"))]\n",
    "qld_files = [os.path.join(data_dir, f\"{well_id}.csv\") for well_id in qld_wells if os.path.isfile(os.path.join(data_dir, f\"{well_id}.csv\"))]\n",
    "print(vic_files)\n",
    "print(qld_files)\n",
    "\n",
    "# Prétraiter les données pour VIC et QLD\n",
    "train_features_tensor_vic, train_target_tensor_vic, test_features_tensor_vic, test_target_tensor_vic, target_scaler_vic = preprocess_region_data(vic_files)\n",
    "train_features_tensor_qld, train_target_tensor_qld, test_features_tensor_qld, test_target_tensor_qld, target_scaler_qld = preprocess_region_data(qld_files)\n",
    "\n",
    "# Créer des DataLoader pour VIC\n",
    "batch_size = 32\n",
    "train_dataset_vic = TensorDataset(train_features_tensor_vic, train_target_tensor_vic)\n",
    "train_loader_vic = DataLoader(train_dataset_vic, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset_vic = TensorDataset(test_features_tensor_vic, test_target_tensor_vic)\n",
    "test_loader_vic = DataLoader(test_dataset_vic, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Créer des DataLoader pour QLD\n",
    "train_dataset_qld = TensorDataset(train_features_tensor_qld, train_target_tensor_qld)\n",
    "train_loader_qld = DataLoader(train_dataset_qld, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset_qld = TensorDataset(test_features_tensor_qld, test_target_tensor_qld)\n",
    "test_loader_qld = DataLoader(test_dataset_qld, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Hyperparamètres pour le fine-tuning\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 50\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning pour VIC\n",
    "model.load_state_dict(torch.load(model_save_path))  # Charger le modèle général\n",
    "model_save_path_vic = './model/vic_model.pth'\n",
    "fine_tune_model(train_loader_vic, model, criterion, optimizer, num_epochs, model_save_path_vic)\n",
    "# Évaluation du modèle fine-tuned sur VIC\n",
    "model.load_state_dict(torch.load('./model/vic_model.pth'))\n",
    "test_mae_vic, std_error_vic, min_error_vic, max_error_vic = evaluate_model(test_loader_vic, model, target_scaler_vic)\n",
    "print(f'VIC - Test MAE: {test_mae_vic:.4f}, Std Error: {std_error_vic:.4f}, Min Error: {min_error_vic:.4f}, Max Error: {max_error_vic:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning pour QLD\n",
    "model.load_state_dict(torch.load(model_save_path))  # Charger le modèle général\n",
    "model_save_path_qld = './model/qld_model.pth'\n",
    "fine_tune_model(train_loader_qld, model, criterion, optimizer, num_epochs, './model/qld_model.pth')\n",
    "\n",
    "model.load_state_dict(torch.load('./model/qld_model.pth'))\n",
    "test_mae_qld, std_error_qld, min_error_qld, max_error_qld = evaluate_model(test_loader_qld, model, target_scaler_qld)\n",
    "print(f'QLD - Test MAE: {test_mae_qld:.4f}, Std Error: {std_error_qld:.4f}, Min Error: {min_error_qld:.4f}, Max Error: {max_error_qld:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèles fine-tuned par puits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèle par region fine-tuned par puits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Prétraitement des données pour une région spécifique\n",
    "def preprocess_single_well_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    data.sort_values('date', inplace=True)\n",
    "\n",
    "    features = data[['P', 'T', 'ET', 'NDVI']]\n",
    "    target = data['GWL']\n",
    "\n",
    "    feature_scaler = MinMaxScaler()\n",
    "    features_scaled = feature_scaler.fit_transform(features)\n",
    "\n",
    "    target_scaler = MinMaxScaler()\n",
    "    target_scaled = target_scaler.fit_transform(target.values.reshape(-1, 1))\n",
    "\n",
    "    sequence_length = 6\n",
    "    num_data_points = len(features_scaled)\n",
    "\n",
    "    train_end = num_data_points - 12\n",
    "    test_start = train_end\n",
    "\n",
    "    if train_end < sequence_length:\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    train_features_list = []\n",
    "    train_target_list = []\n",
    "    test_features_list = []\n",
    "    test_target_list = []\n",
    "\n",
    "    for i in range(sequence_length, train_end + 1):\n",
    "        train_features_list.append(features_scaled[i-sequence_length:i])\n",
    "        train_target_list.append(target_scaled[i-sequence_length+1:i+1])\n",
    "\n",
    "    for i in range(train_end, num_data_points - sequence_length + 1):\n",
    "        test_features_list.append(features_scaled[i-sequence_length:i])\n",
    "        test_target_list.append(target_scaled[i-sequence_length+1:i+1])\n",
    "\n",
    "    train_features_array = np.array(train_features_list)\n",
    "    train_target_array = np.array(train_target_list).reshape(-1, sequence_length)\n",
    "\n",
    "    test_features_array = np.array(test_features_list)\n",
    "    test_target_array = np.array(test_target_list).reshape(-1, sequence_length)\n",
    "\n",
    "    train_features_tensor = torch.tensor(train_features_array, dtype=torch.float32)\n",
    "    train_target_tensor = torch.tensor(train_target_array, dtype=torch.float32)\n",
    "\n",
    "    test_features_tensor = torch.tensor(test_features_array, dtype=torch.float32)\n",
    "    test_target_tensor = torch.tensor(test_target_array, dtype=torch.float32)\n",
    "\n",
    "    return train_features_tensor, train_target_tensor, test_features_tensor, test_target_tensor, target_scaler\n",
    "\n",
    "# Fine-tuning du modèle\n",
    "def fine_tune_model(train_loader, model, criterion, optimizer, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for features, target in train_loader:\n",
    "            output = model(features)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}')\n",
    "\n",
    "# Évaluation du modèle sur les données de test\n",
    "def evaluate_model(test_loader, model, target_scaler):\n",
    "    model.eval()\n",
    "    mae = 0\n",
    "    with torch.no_grad():\n",
    "        for features, target in test_loader:\n",
    "            output = model(features)\n",
    "            output = target_scaler.inverse_transform(output.numpy())\n",
    "            target = target_scaler.inverse_transform(target.numpy())\n",
    "\n",
    "            mae += np.mean(np.abs(output - target))\n",
    "    \n",
    "    mae /= len(test_loader)\n",
    "    return mae\n",
    "\n",
    "# Script principal\n",
    "data_dir = './data1'\n",
    "model_save_path_vic = './model/vic_model.pth'\n",
    "model_save_path_qld = './model/qld_model.pth'\n",
    "\n",
    "# Charger les modèles pré-entraînés pour chaque région\n",
    "input_size = 4  # Nombre de caractéristiques d'entrée (P, T, ET, NDVI)\n",
    "hidden_size = 50\n",
    "num_layers = 2\n",
    "output_size = 6  # Prédiction de 6 mois\n",
    "\n",
    "model_vic = LSTMModel(input_size, hidden_size, num_layers, output_size)\n",
    "model_vic.load_state_dict(torch.load(model_save_path_vic))\n",
    "\n",
    "model_qld = LSTMModel(input_size, hidden_size, num_layers, output_size)\n",
    "model_qld.load_state_dict(torch.load(model_save_path_qld))\n",
    "\n",
    "# Lire OUVRAGES.csv pour obtenir les identifiants de puits et les régions\n",
    "ouvrages_file_path = './OUVRAGES.csv'\n",
    "ouvrages_data = pd.read_csv(ouvrages_file_path)\n",
    "\n",
    "# Identifier les puits pour VIC et QLD\n",
    "well_id_column = 'Ouvrage'\n",
    "region_column = 'Region'\n",
    "\n",
    "vic_wells = ouvrages_data[ouvrages_data[region_column] == 'VIC'][well_id_column].tolist()\n",
    "qld_wells = ouvrages_data[ouvrages_data[region_column] == 'QLD'][well_id_column].tolist()\n",
    "\n",
    "vic_files = [os.path.join(data_dir, f\"{well_id}.csv\") for well_id in vic_wells if os.path.isfile(os.path.join(data_dir, f\"{well_id}.csv\"))]\n",
    "qld_files = [os.path.join(data_dir, f\"{well_id}.csv\") for well_id in qld_wells if os.path.isfile(os.path.join(data_dir, f\"{well_id}.csv\"))]\n",
    "\n",
    "\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fichier pour sauvegarder les résultats\n",
    "results_file_path = './results/fine_tuning_results_vic.txt'\n",
    "if not os.path.exists(os.path.dirname(results_file_path)):\n",
    "    os.makedirs(os.path.dirname(results_file_path))\n",
    "    \n",
    "# Fine-tuning pour chaque puits dans VIC\n",
    "mae_list_vic = []\n",
    "with open(results_file_path, 'w') as f:\n",
    "    for file_path in vic_files:\n",
    "        preprocessed_data = preprocess_single_well_data(file_path)\n",
    "        if preprocessed_data[0] is None:\n",
    "            continue\n",
    "\n",
    "        train_features_tensor, train_target_tensor, test_features_tensor, test_target_tensor, target_scaler = preprocessed_data\n",
    "\n",
    "        # Créer des DataLoader\n",
    "        train_dataset = TensorDataset(train_features_tensor, train_target_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        test_dataset = TensorDataset(test_features_tensor, test_target_tensor)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n",
    "        model.load_state_dict(model_vic.state_dict()) \n",
    "\n",
    "        criterion = nn.L1Loss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Entraîner le modèle pour ce puits\n",
    "        well_id = os.path.basename(file_path).split('.')[0]\n",
    "\n",
    "        print(f'Training model for well: {well_id} in VIC')\n",
    "        fine_tune_model(train_loader, model, criterion, optimizer, num_epochs)\n",
    "\n",
    "        # Évaluer le modèle entraîné sur ce puits\n",
    "        test_mae = evaluate_model(test_loader, model, target_scaler)\n",
    "        mae_list_vic.append(test_mae)\n",
    "        print(f'VIC Well {well_id} - Test MAE: {test_mae:.4f}')\n",
    "\n",
    "        # Sauvegarder\n",
    "        f.write(f'VIC Well {well_id} - Test MAE: {test_mae:.4f}\\n')\n",
    "\n",
    "    # Calculer la moyenne des MAE pour VIC\n",
    "    avg_mae_vic = np.mean(mae_list_vic)\n",
    "    print(f'Average MAE for VIC: {avg_mae_vic:.4f}')\n",
    "    f.write(f'Average MAE for VIC: {avg_mae_vic:.4f}\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fichier pour sauvegarder les résultats\n",
    "results_file_path = './results/fine_tuning_results_qld.txt'\n",
    "if not os.path.exists(os.path.dirname(results_file_path)):\n",
    "    os.makedirs(os.path.dirname(results_file_path))\n",
    "\n",
    "# Fine-tuning pour chaque puits dans QLD\n",
    "mae_list_qld = []\n",
    "with open(results_file_path, 'a') as f:\n",
    "    for file_path in qld_files:\n",
    "        preprocessed_data = preprocess_single_well_data(file_path)\n",
    "        if preprocessed_data[0] is None:\n",
    "            continue\n",
    "\n",
    "        train_features_tensor, train_target_tensor, test_features_tensor, test_target_tensor, target_scaler = preprocessed_data\n",
    "\n",
    "        # Créer des DataLoader\n",
    "        train_dataset = TensorDataset(train_features_tensor, train_target_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        test_dataset = TensorDataset(test_features_tensor, test_target_tensor)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Charger les poids du modèle pré-entraîné QLD\n",
    "        model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n",
    "        model.load_state_dict(model_qld.state_dict())  # Initialiser avec les poids du modèle QLD\n",
    "\n",
    "        criterion = nn.L1Loss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Entraîner le modèle pour ce puits\n",
    "        well_id = os.path.basename(file_path).split('.')[0]\n",
    "\n",
    "        print(f'Training model for well: {well_id} in QLD')\n",
    "        fine_tune_model(train_loader, model, criterion, optimizer, num_epochs)\n",
    "\n",
    "        # Évaluer le modèle entraîné sur ce puits\n",
    "        test_mae = evaluate_model(test_loader, model, target_scaler)\n",
    "        mae_list_qld.append(test_mae)\n",
    "        print(f'QLD Well {well_id} - Test MAE: {test_mae:.4f}')\n",
    "\n",
    "        # Sauvegarder\n",
    "        f.write(f'QLD Well {well_id} - Test MAE: {test_mae:.4f}\\n')\n",
    "\n",
    "    # Calculer la moyenne des MAE pour QLD\n",
    "    avg_mae_qld = np.mean(mae_list_qld)\n",
    "    print(f'Average MAE for QLD: {avg_mae_qld:.4f}')\n",
    "    f.write(f'Average MAE for QLD: {avg_mae_qld:.4f}\\n\\n')\n",
    "\n",
    "print(f'All results saved to {results_file_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèle genéral fine-tuned par puits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Prétraiter les données pour un seul puits\n",
    "def preprocess_single_well_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    data.sort_values('date', inplace=True)\n",
    "\n",
    "    features = data[['P', 'T', 'ET', 'NDVI']]\n",
    "    target = data['GWL']\n",
    "\n",
    "    feature_scaler = MinMaxScaler()\n",
    "    features_scaled = feature_scaler.fit_transform(features)\n",
    "\n",
    "    target_scaler = MinMaxScaler()\n",
    "    target_scaled = target_scaler.fit_transform(target.values.reshape(-1, 1))\n",
    "\n",
    "    sequence_length = 6\n",
    "    num_data_points = len(features_scaled)\n",
    "    train_end = num_data_points - 12\n",
    "\n",
    "    if train_end < sequence_length:\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    train_features_list = []\n",
    "    train_target_list = []\n",
    "    test_features_list = []\n",
    "    test_target_list = []\n",
    "\n",
    "    for i in range(sequence_length, train_end + 1):\n",
    "        train_features_list.append(features_scaled[i-sequence_length:i])\n",
    "        train_target_list.append(target_scaled[i+sequence_length-6:i+sequence_length])\n",
    "\n",
    "    for i in range(train_end, num_data_points - sequence_length + 1):\n",
    "        test_features_list.append(features_scaled[i-sequence_length:i])\n",
    "        test_target_list.append(target_scaled[i+sequence_length-6:i+sequence_length])\n",
    "\n",
    "    train_features_array = np.array(train_features_list)\n",
    "    train_target_array = np.array(train_target_list).reshape(-1, 6)\n",
    "\n",
    "    test_features_array = np.array(test_features_list)\n",
    "    test_target_array = np.array(test_target_list).reshape(-1, 6)\n",
    "\n",
    "    train_features_tensor = torch.tensor(train_features_array, dtype=torch.float32)\n",
    "    train_target_tensor = torch.tensor(train_target_array, dtype=torch.float32)\n",
    "    test_features_tensor = torch.tensor(test_features_array, dtype=torch.float32)\n",
    "    test_target_tensor = torch.tensor(test_target_array, dtype=torch.float32)\n",
    "\n",
    "    return train_features_tensor, train_target_tensor, test_features_tensor, test_target_tensor, target_scaler\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Entraîner le modèle\n",
    "def train_model(train_loader, model, criterion, optimizer, num_epochs):\n",
    "    train_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for features, target in train_loader:\n",
    "            output = model(features)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}')\n",
    "    \n",
    "    return train_losses\n",
    "\n",
    "# Évaluer le modèle\n",
    "def evaluate_model(test_loader, model, target_scaler):\n",
    "    model.eval()\n",
    "    mae = 0\n",
    "    with torch.no_grad():\n",
    "        for features, target in test_loader:\n",
    "            output = model(features)\n",
    "            output = target_scaler.inverse_transform(output.numpy())\n",
    "            target = target_scaler.inverse_transform(target.numpy())\n",
    "\n",
    "            mae += np.mean(np.abs(output - target))\n",
    "    \n",
    "    mae /= len(test_loader)\n",
    "    return mae\n",
    "\n",
    "\n",
    "data_dir = './data1'\n",
    "files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.csv')]\n",
    "\n",
    "# Charger le modèle général pré-entraîné\n",
    "model_save_path_general = './model/best_model.pth'\n",
    "input_size = 4\n",
    "hidden_size = 50\n",
    "num_layers = 2\n",
    "output_size = 6\n",
    "\n",
    "model_general = LSTMModel(input_size, hidden_size, num_layers, output_size)\n",
    "model_general.load_state_dict(torch.load(model_save_path_general))\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "\n",
    "# Fichier pour sauvegarder les résultats\n",
    "results_file_path = './results/fine_tuning_results.txt'\n",
    "if not os.path.exists(os.path.dirname(results_file_path)):\n",
    "    os.makedirs(os.path.dirname(results_file_path))\n",
    "\n",
    "mae_list = []\n",
    "\n",
    "with open(results_file_path, 'w') as f:\n",
    "    for file_path in files:\n",
    "        preprocessed_data = preprocess_single_well_data(file_path)\n",
    "        if preprocessed_data[0] is None:\n",
    "            continue\n",
    "\n",
    "        train_features_tensor, train_target_tensor, test_features_tensor, test_target_tensor, target_scaler = preprocessed_data\n",
    "\n",
    "        train_dataset = TensorDataset(train_features_tensor, train_target_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        test_dataset = TensorDataset(test_features_tensor, test_target_tensor)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n",
    "        model.load_state_dict(model_general.state_dict())\n",
    "\n",
    "        criterion = nn.L1Loss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        well_id = os.path.basename(file_path).split('.')[0]\n",
    "\n",
    "        print(f'Training model for well: {well_id}')\n",
    "        train_losses = train_model(train_loader, model, criterion, optimizer, num_epochs)\n",
    "\n",
    "        test_mae = evaluate_model(test_loader, model, target_scaler)\n",
    "        mae_list.append(test_mae)\n",
    "        print(f'Well {well_id} - Test MAE: {test_mae:.4f}')\n",
    "\n",
    "        f.write(f'Well {well_id} - Test MAE: {test_mae:.4f}\\n')\n",
    "        f.write(f'Train Losses: {train_losses}\\n\\n')\n",
    "\n",
    "    avg_mae = np.mean(mae_list)\n",
    "    print(f'Average MAE: {avg_mae:.4f}')\n",
    "    f.write(f'Average MAE: {avg_mae:.4f}\\n')\n",
    "\n",
    "print(f'All results saved to {results_file_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
